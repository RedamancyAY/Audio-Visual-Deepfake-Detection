{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e94e3ea-1a71-4e71-a3dc-021d144bfc04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T14:14:52.419261Z",
     "iopub.status.busy": "2023-04-13T14:14:52.418565Z",
     "iopub.status.idle": "2023-04-13T14:14:52.658046Z",
     "shell.execute_reply": "2023-04-13T14:14:52.657604Z",
     "shell.execute_reply.started": "2023-04-13T14:14:52.419139Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import python_speech_features\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import decode_png, encode_png, read_image, read_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd2494-04b8-4adc-add0-b3f2870d3faf",
   "metadata": {},
   "source": [
    "# Read Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422bde83-97b7-44e2-b07d-16c0ec611441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read_video(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_frames, img_size=(224, 224), face_detect_method=None, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_frames = n_frames\n",
    "        if type(img_size) is int:\n",
    "            img_size = (img_size, img_size)\n",
    "        self.resize = T.Resize(img_size)\n",
    "        self.face_detect_method = face_detect_method\n",
    "\n",
    "    def read_on_sec(self, video_path, sec_id, video_fps=30):\n",
    "        frames_sep = video_fps / self.n_frames\n",
    "\n",
    "        frame_ids = [\n",
    "            int((sec_id - 1) * video_fps + i * frames_sep) for i in range(self.n_frames)\n",
    "        ]\n",
    "        # print(frame_ids, sec_id, video_fps)\n",
    "\n",
    "        x = torch.stack(\n",
    "            [self.read_frame_of_video(video_path, frame_id) for frame_id in frame_ids]\n",
    "        )\n",
    "\n",
    "        if x.shape[0] < self.n_frames:\n",
    "            T, C, H, W = x.shape\n",
    "            x = torch.concat(\n",
    "                [x, torch.zeros((self.n_frames - T, C, H, W), dtype=torch.float32)],\n",
    "                dim=0,\n",
    "            )\n",
    "        x = self.resize(x)\n",
    "        return x.contiguous()\n",
    "\n",
    "    def read_frame_of_video(self, video_path, frame_id):\n",
    "        return read_image(os.path.join(video_path, \"%04d.png\" % (frame_id + 1)))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        video_path,\n",
    "        frame_id=-1,\n",
    "        sec_id=-1,\n",
    "        video_fps=25,\n",
    "        video_total_frames=75,\n",
    "        start_sec=0,\n",
    "        end_sec=3,\n",
    "    ):\n",
    "        # read frame\n",
    "        if frame_id != -1:\n",
    "            frames_sep = min(video_total_frames, video_fps * 3) / self.n_frames\n",
    "            frame_ids = [int(i * frames_sep) for i in range(self.n_frames)]\n",
    "            image = self.read_frame_of_video(\n",
    "                video_path, frame_ids[frame_id] + start_sec * video_fps\n",
    "            )\n",
    "            return self.resize(image)\n",
    "\n",
    "        # print(video_fps, video_total_frames, sec_id)\n",
    "        # read second\n",
    "        if sec_id != -1:\n",
    "            return self.read_on_sec(video_path, sec_id + start_sec, video_fps=video_fps)\n",
    "\n",
    "        # read 10 frames from the first 3 seconds\n",
    "        if video_total_frames < self.n_frames:\n",
    "            x = torch.stack(\n",
    "                [\n",
    "                    self.read_frame_of_video(video_path, i)\n",
    "                    for i in range(video_total_frames)\n",
    "                ]\n",
    "            )\n",
    "            T, C, H, W = x.shape\n",
    "            x = torch.concat(\n",
    "                [x, torch.zeros(self.n_frames - video_total_frames, C, H, W)], dim=0\n",
    "            )\n",
    "        else:\n",
    "            video_total_frames = min(video_total_frames, video_fps * 3)\n",
    "            frames_sep = video_total_frames / self.n_frames\n",
    "            x = torch.stack(\n",
    "                [\n",
    "                    self.read_frame_of_video(\n",
    "                        video_path, int(i * frames_sep) + start_sec * video_fps\n",
    "                    )\n",
    "                    for i in range(self.n_frames)\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "            # print([int(i * frames_sep) for i in range(self.n_frames)])\n",
    "\n",
    "        x = self.resize(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a04e3d-c461-4e58-aba7-69b160917422",
   "metadata": {
    "scrolled": true,
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [],
   "source": [
    "reader = Read_video(n_frames=10, img_size=224)\n",
    "\n",
    "video_path = \"/usr/local/ay_data/dataset/Celeb-DF-v2/Celeb-real/id0_0003.mp4\"\n",
    "reader(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d01ee0-f5ca-4915-8d90-ad43e30a7f1f",
   "metadata": {},
   "source": [
    "# Read Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa93d0e-7262-4ed7-a799-5da97078d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(_audio, _sr):\n",
    "    mfcc = zip(*python_speech_features.mfcc(_audio, _sr, nfft=2048))\n",
    "    mfcc = np.stack([np.array(i) for i in mfcc])\n",
    "    cc = np.expand_dims(np.expand_dims(mfcc, axis=0), axis=0)\n",
    "    # print(cc.shape)\n",
    "    return torch.tensor(cc, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15960358-3400-4596-975b-528fc6baa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read_audio(nn.Module):\n",
    "    def __init__(self, freq, length, features=None):\n",
    "        super().__init__()\n",
    "        self.length = int(length)\n",
    "        self.freq = freq\n",
    "        self.features = features\n",
    "\n",
    "    def read_audio(self, audio_path):\n",
    "        x, sample_rate = torchaudio.load(audio_path)\n",
    "        if x.shape[0] > 1:\n",
    "            x = x[0:1, :]\n",
    "        if sample_rate != self.freq:\n",
    "            x = torchaudio.functional.resample(x, sample_rate, self.freq)\n",
    "        length = x.size(1)\n",
    "        if length >= self.length:\n",
    "            return x[:, : self.length]\n",
    "        else:\n",
    "            return torch.concatenate([x, torch.zeros(1, self.length - length)], dim=1)\n",
    "\n",
    "    def read_waveform(self, audio_path, sec_id=-1):\n",
    "        x = self.read_audio(audio_path)\n",
    "        if sec_id != -1:\n",
    "            x = x[:, self.freq * (sec_id - 1) : self.freq * sec_id]\n",
    "        return x\n",
    "\n",
    "    def read_features(self, audio_path, sec_id=-1):\n",
    "        x, sr = torchaudio.backend.sox_io_backend.load(audio_path, normalize=False)\n",
    "        if x.shape[0] > 1:\n",
    "            x = x[0:1, :]\n",
    "        if sr != self.freq:\n",
    "            x = torchaudio.functional.resample(x, sr, self.freq)\n",
    "        if sec_id != -1:\n",
    "            x = x[:, self.freq * (sec_id - 1) : self.freq * sec_id]\n",
    "        length = x.size(1)\n",
    "        if length >= self.freq:\n",
    "            x = x[:, : self.freq]\n",
    "        else:\n",
    "            x = torch.concatenate([x, torch.zeros(1, self.freq - length)], dim=1)\n",
    "\n",
    "        return get_mfcc(x[0].numpy(), self.freq)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        audio_path,\n",
    "        sec_id=-1,\n",
    "        start_sec=0,\n",
    "        end_sec=3,\n",
    "    ):\n",
    "        if self.features is None:\n",
    "            return self.read_waveform(audio_path, sec_id=sec_id)\n",
    "        else:\n",
    "            return self.read_features(audio_path, sec_id=sec_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54727304-d060-4bff-8871-5c9c88907356",
   "metadata": {
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "reader = Read_audio(freq=16000, length=48000, features=\"mfcc\")\n",
    "video_path = \"/home/ay/data/DATA/dataset/0-deepfake/FakeAVCeleb_v1.2/FakeVideo-FakeAudio/African/women/id00359/00053_id04376_wavtolip.mp4\"\n",
    "reader(video_path)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
