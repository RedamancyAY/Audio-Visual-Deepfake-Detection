{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1929b8d-374f-4c78-b1ba-2bb2f9ee6a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T01:09:46.649499Z",
     "iopub.status.busy": "2023-03-13T01:09:46.648714Z",
     "iopub.status.idle": "2023-03-13T01:09:46.677244Z",
     "shell.execute_reply": "2023-03-13T01:09:46.676314Z",
     "shell.execute_reply.started": "2023-03-13T01:09:46.649312Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e545e99-ee8f-45a4-836f-1f5745edd072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T01:09:47.512034Z",
     "iopub.status.busy": "2023-03-13T01:09:47.511036Z",
     "iopub.status.idle": "2023-03-13T01:09:48.260068Z",
     "shell.execute_reply": "2023-03-13T01:09:48.259378Z",
     "shell.execute_reply.started": "2023-03-13T01:09:47.511993Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import hashlib\n",
    "import os\n",
    "import pathlib\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990e617-d414-4345-9749-0095798a6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .preprocessing import Read_audio, Read_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372ffa7f-c366-47d9-a528-85f523534bf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T05:30:17.055468Z",
     "iopub.status.busy": "2023-03-13T05:30:17.054391Z",
     "iopub.status.idle": "2023-03-13T05:30:17.084516Z",
     "shell.execute_reply": "2023-03-13T05:30:17.083873Z",
     "shell.execute_reply.started": "2023-03-13T05:30:17.055422Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "\n",
    "from .image_compression import JPEGCompression\n",
    "\n",
    "\n",
    "def get_video_aug_func(cfg):\n",
    "    res = []\n",
    "    format_info = \"Image augumentaion: \"\n",
    "    if cfg.jpeg_compression:\n",
    "        res.append(JPEGCompression(quality_lower=60, quality_upper=100, p=0.5))\n",
    "        format_info += \"jpeg compression, \"\n",
    "\n",
    "    if len(res) > 0:\n",
    "        print(format_info)\n",
    "        return Compose(res)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebfd34-fdc8-46fb-94f8-30bd8887b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from audiomentations import TimeStretch as Org_TimeStretch\n",
    "\n",
    "\n",
    "class RandomAlign:\n",
    "    def __init__(self, p=0.5, max_length=18000, min_length=4500):\n",
    "        self.p = p\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, audio_data):\n",
    "        if random.random() > self.p:\n",
    "            return audio_data\n",
    "\n",
    "        L = audio_data.shape[-1]\n",
    "        align_length = random.randint(self.min_length, self.max_length)\n",
    "        align = torch.zeros(1, align_length)\n",
    "        if random.random() > 0.5:\n",
    "            audio_data = torch.concat([audio_data[:, align_length:], align], dim=-1)\n",
    "        else:\n",
    "            audio_data = torch.concat(\n",
    "                [align, audio_data[:, 0 : L - align_length]], dim=-1\n",
    "            )\n",
    "        return audio_data\n",
    "\n",
    "\n",
    "class TimeStretch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        leave_length_unchanged=True,\n",
    "        p=1.0,\n",
    "        sample_rate=16000,\n",
    "    ):\n",
    "        self.transform = Org_TimeStretch(\n",
    "            min_rate=min_rate,\n",
    "            max_rate=max_rate,\n",
    "            leave_length_unchanged=leave_length_unchanged,\n",
    "            p=p,\n",
    "        )\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __call__(self, audio, sample_rate=-1):\n",
    "        if sample_rate == -1:\n",
    "            sample_rate = self.sample_rate\n",
    "\n",
    "        if type(audio) == np.ndarray:\n",
    "            return self.transform(audio, sample_rate=sample_rate)\n",
    "        else:\n",
    "            audio = self.transform(audio.numpy(), sample_rate=sample_rate)\n",
    "            return torch.from_numpy(audio)\n",
    "\n",
    "\n",
    "def get_audio_aug_func(cfg):\n",
    "\n",
    "    res = []\n",
    "    format_info = \"Audio augumentaion: \"\n",
    "    if cfg.random_speed:\n",
    "        res.append(\n",
    "            TimeStretch(\n",
    "                min_rate=0.8,\n",
    "                max_rate=1.25,\n",
    "                leave_length_unchanged=True,\n",
    "                p=1.0,\n",
    "                sample_rate=16000,\n",
    "            )\n",
    "        )\n",
    "        format_info += \"Random Speed\"\n",
    "\n",
    "    if cfg.random_align:\n",
    "        res.append(RandomAlign(p=0.5, min_length=4000, max_length=8000))\n",
    "        format_info += \"Random align\"\n",
    "\n",
    "    if len(res) > 0:\n",
    "        print(format_info)\n",
    "        return Compose(res)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33418cba-2b7d-4571-893a-c631659852e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T05:30:21.132413Z",
     "iopub.status.busy": "2023-03-13T05:30:21.131818Z",
     "iopub.status.idle": "2023-03-13T05:30:21.176193Z",
     "shell.execute_reply": "2023-03-13T05:30:21.175404Z",
     "shell.execute_reply.started": "2023-03-13T05:30:21.132375Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeepFake_Dataset(Dataset):\n",
    "    \"\"\"Torch.utils.Dataset\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, data, cfg, cfg_aug=None, train=True, custom_collect_fn=None, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.use_audio = cfg.use_audio\n",
    "        self.train_on_frame = cfg.train_on_frame\n",
    "        self.train_on_sec = cfg.train_on_sec\n",
    "        self.train_on_mouth = cfg.train_on_mouth\n",
    "        self.cfg = cfg\n",
    "        if self.train_on_mouth:\n",
    "\n",
    "            def check_mouth(x):\n",
    "                mouth_path = x[\"mouth_path\"]\n",
    "                n_frames = x[\"n_frames\"]\n",
    "                if not os.path.exists(mouth_path):\n",
    "                    return 0\n",
    "                # pngs = [x for x in os.listdir(mouth_path) if x.endswith('png')]\n",
    "                # if len(pngs) == n_frames:\n",
    "                #     return 1\n",
    "                # else:\n",
    "                #     return 0\n",
    "                return 1\n",
    "\n",
    "            self.data[\"mouth\"] = self.data.apply(check_mouth, axis=1)\n",
    "            len1 = len(self.data)\n",
    "            self.data = self.data[self.data[\"mouth\"] == 1].reset_index(drop=True)\n",
    "            len2 = len(self.data)\n",
    "            print(\"Filter the videos that does not have mouth: \", len1, \" to \", len2)\n",
    "\n",
    "        self.read_audio_func = Read_audio(\n",
    "            freq=cfg.audio_freq,\n",
    "            length=cfg.audio_freq * 3,\n",
    "            features=cfg.audio_features,\n",
    "        )\n",
    "        self.read_video_func = Read_video(\n",
    "            n_frames=cfg.video_n_frames,\n",
    "            img_size=cfg.video_img_size,\n",
    "            face_detect_method=cfg.face_detect_method,\n",
    "            train_frame=cfg.train_on_frame,\n",
    "            train_sec=cfg.train_on_sec,\n",
    "        )\n",
    "        print(len(data), train)\n",
    "        self.video_aug_func = None if cfg_aug is None else get_video_aug_func(cfg_aug)\n",
    "        self.audio_aug_func = None if cfg_aug is None else get_audio_aug_func(cfg_aug)\n",
    "\n",
    "        self.train = train\n",
    "        # from ay.torch.transforms.audio import RandomBackgroundNoise\n",
    "        # self.noise_transform = RandomBackgroundNoise(\n",
    "        #     sample_rate=16000, noise_dir=\"/home/ay/musan/noise\"\n",
    "        # )\n",
    "        # print(self.video_aug_func, self.noise_transform)\n",
    "\n",
    "        self.custom_collect_fn = custom_collect_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_video(self, video):\n",
    "        if self.video_aug_func is not None:\n",
    "            video = self.video_aug_func(video)\n",
    "        if type(video) is np.ndarray:\n",
    "            # video = torch.tensor(video.copy())\n",
    "            video = torch.from_numpy(video)\n",
    "        if len(video.shape) == 4:\n",
    "            video = video.transpose(0, 1)\n",
    "        return video / 255\n",
    "\n",
    "    def process_audio(self, audio):\n",
    "        if self.audio_aug_func is not None:\n",
    "            audio = self.audio_aug_func(audio)\n",
    "        return audio\n",
    "\n",
    "    def get_labels(self, item):\n",
    "        labels = {\"video_label\": item[\"video_label\"]}\n",
    "        if \"audio_label\" in item.keys():\n",
    "            labels[\"audio_label\"] = item[\"audio_label\"]\n",
    "        if \"label\" in item.keys():\n",
    "            labels[\"label\"] = item[\"label\"]\n",
    "        return labels\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        item = self.data.iloc[ndx]\n",
    "        labels = self.get_labels(item)\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        frame_id = (\n",
    "            self.data.iloc[ndx][\"frame_id\"]\n",
    "            if self.train_on_frame and self.train\n",
    "            else -1\n",
    "        )\n",
    "        sec_id = self.data.iloc[ndx][\"sec_id\"] if self.train_on_sec else -1\n",
    "\n",
    "        if self.train_on_mouth:\n",
    "            video_path = item[\"mouth_path\"]\n",
    "        else:\n",
    "            video_path = item[\"video_path\"]\n",
    "\n",
    "        start_sec = item[\"start_sec\"] if \"start_sec\" in item.keys() else 0\n",
    "        end_sec = item[\"end_sec\"] if \"end_sec\" in item.keys() else 3\n",
    "\n",
    "        _data = self.process_video(\n",
    "            self.read_video_func(\n",
    "                video_path,\n",
    "                frame_id=frame_id,\n",
    "                sec_id=sec_id,\n",
    "                video_fps=item[\"fps\"],\n",
    "                video_total_frames=item[\"n_frames\"],\n",
    "                start_sec=start_sec,\n",
    "                end_sec=end_sec,\n",
    "            )\n",
    "        )  # (T, C, H, W)\n",
    "        if frame_id == -1:\n",
    "            data[\"video\"] = _data\n",
    "        else:\n",
    "            data[\"frame\"] = _data\n",
    "            # print(data['frame'].shape)\n",
    "\n",
    "        if self.use_audio:\n",
    "            data[\"audio\"] = self.process_audio(\n",
    "                self.read_audio_func(\n",
    "                    item[\"audio_path\"],\n",
    "                    sec_id=sec_id,\n",
    "                    start_sec=start_sec,\n",
    "                    end_sec=end_sec,\n",
    "                )\n",
    "            )\n",
    "        # print(data['video'].shape, data['audio'].shape)\n",
    "        data[\"video_path\"] = video_path\n",
    "        # for key in data.keys():\n",
    "        # print(key, data[key].shape if type(data[key]) is not str else 0)\n",
    "        # print(data['video'].dtype, data['audio'].dtype, data['video'].shape, data['audio'].shape)\n",
    "        if self.custom_collect_fn is None:\n",
    "            return data, labels\n",
    "        else:\n",
    "            return self.custom_collect_fn(\n",
    "                data,\n",
    "                labels,\n",
    "                self.cfg.video_n_frames,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
