{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a35bfd-9905-40f0-8e5b-a5c84dc9f657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T12:56:31.214387Z",
     "iopub.status.busy": "2023-03-07T12:56:31.213849Z",
     "iopub.status.idle": "2023-03-07T12:56:31.238963Z",
     "shell.execute_reply": "2023-03-07T12:56:31.238239Z",
     "shell.execute_reply.started": "2023-03-07T12:56:31.214277Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d70146-edc4-48ab-8754-a2a1b3d1067a",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff03d9b-e2f1-4cd9-830a-eef470ffa58f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:03:10.324907Z",
     "iopub.status.busy": "2023-03-07T13:03:10.324395Z",
     "iopub.status.idle": "2023-03-07T13:03:10.354147Z",
     "shell.execute_reply": "2023-03-07T13:03:10.353312Z",
     "shell.execute_reply.started": "2023-03-07T13:03:10.324865Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from einops import rearrange\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f401096-3b13-47d2-bc77-104d9c25850a",
   "metadata": {
    "tags": [
     "style-activity"
    ]
   },
   "outputs": [],
   "source": [
    "from .utils import Multi_Head_Attention, weight_init, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c33ea236-ecd8-4a0d-bd29-85cfec22828e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:01:24.761223Z",
     "iopub.status.busy": "2023-03-07T13:01:24.760713Z",
     "iopub.status.idle": "2023-03-07T13:01:24.785910Z",
     "shell.execute_reply": "2023-03-07T13:01:24.785102Z",
     "shell.execute_reply.started": "2023-03-07T13:01:24.761178Z"
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "from utils import Multi_Head_Attention, weight_init, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b32986-33c5-4c08-be79-6789f0e412ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:03:12.135069Z",
     "iopub.status.busy": "2023-03-07T13:03:12.133979Z",
     "iopub.status.idle": "2023-03-07T13:03:12.177778Z",
     "shell.execute_reply": "2023-03-07T13:03:12.176900Z",
     "shell.execute_reply.started": "2023-03-07T13:03:12.135020Z"
    }
   },
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_dim,\n",
    "        audio_dim,\n",
    "        window_size=7,\n",
    "        n_frames=10,\n",
    "        audio_length=None,\n",
    "        video_size=None,\n",
    "        drop_path=0.1,\n",
    "        attn_dropout=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "        \n",
    "        self.min_dim = 1\n",
    "\n",
    "        ## deal video\n",
    "        self.conv11 = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                video_dim,\n",
    "                video_dim,\n",
    "                groups=video_dim,\n",
    "                kernel_size=(1, 3, 3),\n",
    "                padding=(0, 1, 1),\n",
    "            ),\n",
    "            nn.BatchNorm3d(video_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(\n",
    "                video_dim, self.min_dim, kernel_size=(1, 3, 3), padding=(0, 1, 1)\n",
    "            ),\n",
    "        )\n",
    "        self.conv12 = nn.Conv3d(\n",
    "            self.min_dim, video_dim, kernel_size=(1, 3, 3), padding=(0, 1, 1)\n",
    "        )\n",
    "        self.window_size = window_size\n",
    "        self.n_frames = n_frames\n",
    "        self.video_dim = video_dim\n",
    "\n",
    "        ## deal audio\n",
    "        self.conv21 = nn.Sequential(\n",
    "            nn.Conv1d(audio_dim, audio_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(audio_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(audio_dim, self.min_dim, kernel_size=33, stride=1, padding=16),\n",
    "        )\n",
    "        self.conv22 = nn.Conv1d(\n",
    "            self.min_dim, audio_dim, kernel_size=33, stride=1, padding=16\n",
    "        )\n",
    "        ## self-attention\n",
    "        num_embeddings = (video_size**2) // (window_size**2) * n_frames + n_frames\n",
    "        # print('num_embeddings is ', num_embeddings, video_size, window_size)\n",
    "        self.attention = Multi_Head_Attention(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embed_dim=window_size**2 * self.min_dim,\n",
    "            num_heads=self.min_dim,\n",
    "            QKV=False,\n",
    "            projection=True,\n",
    "            dropout=attn_dropout\n",
    "        )\n",
    "\n",
    "        ## PS. layer scaling\n",
    "        alpha_0 = 1e-2\n",
    "        self.alpha_1 = nn.Parameter(\n",
    "            alpha_0 * torch.ones((video_dim)), requires_grad=True\n",
    "        )\n",
    "        self.alpha_2 = nn.Parameter(\n",
    "            alpha_0 * torch.ones((audio_dim)), requires_grad=True\n",
    "        )\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, video, audio, grad_cam=False):\n",
    "        ## 1. deal video\n",
    "        x = self.conv11(video)\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"b c t (p1 h) (p2 w) -> b (t p1 p2) (c h w)\",\n",
    "            h=self.window_size,\n",
    "            w=self.window_size,\n",
    "        )\n",
    "        # print(\"downsmaple x\", x.shape)\n",
    "\n",
    "        ## 2. deal audio\n",
    "        y = self.conv21(audio)\n",
    "        y = rearrange(\n",
    "            y,\n",
    "            \"b c (n l) -> b (n c) l\",\n",
    "            n=self.n_frames,\n",
    "            l=audio.shape[-1] // self.n_frames,\n",
    "        )\n",
    "        audio_length = y.shape[-1]\n",
    "        y = F.adaptive_avg_pool1d(y, 49)\n",
    "        y = rearrange(y, \"b (n c) l -> b n (c l)\", c=self.min_dim)\n",
    "        # print(\"downsmaple y\", y.shape)\n",
    "\n",
    "        ## 3. self-attention\n",
    "        # print(x.shape, y.shape)\n",
    "        z = torch.concat([x, y], dim=1)\n",
    "        z = z + self.attention(z)\n",
    "\n",
    "        ## 4. recover video and audio\n",
    "        x, y = z[:, : x.shape[1], :], z[:, x.shape[1] :, :]\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"b (t p1 p2) (c h w) -> b c t (h p1) (w p2)\",\n",
    "            t=self.n_frames,\n",
    "            c=self.min_dim,\n",
    "            p1=video.shape[-1] // self.window_size,\n",
    "            p2=video.shape[-1] // self.window_size,\n",
    "            h=self.window_size,\n",
    "            w=self.window_size,\n",
    "        )\n",
    "        x = self.drop_path(self.alpha_1.view(-1, 1, 1, 1) * self.conv12(x)) + video\n",
    "        # print(x.shape)\n",
    "\n",
    "        y = rearrange(y, \"b n (c l) -> b (n c) l\", c=self.min_dim)\n",
    "        y = F.interpolate(y, size=audio_length)\n",
    "        y = rearrange(y, \"b (n c) l -> b c (n l)\", c=self.min_dim)\n",
    "        y = self.drop_path(self.alpha_2.view(-1, 1) * self.conv22(y)) + audio\n",
    "        # print(y.shape)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be8e33cd-fee4-4d43-a273-9927fb7b8a49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:03:26.788719Z",
     "iopub.status.busy": "2023-03-07T13:03:26.788233Z",
     "iopub.status.idle": "2023-03-07T13:03:26.943848Z",
     "shell.execute_reply": "2023-03-07T13:03:26.943212Z",
     "shell.execute_reply.started": "2023-03-07T13:03:26.788678Z"
    },
    "tags": [
     "active-ipynb",
     "style-student"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 10, 56, 56]) torch.Size([2, 32, 12000])\n",
      "35551\n",
      "torch.Size([650])\n",
      "torch.Size([650])\n",
      "torch.Size([650, 49])\n",
      "torch.Size([49, 49])\n"
     ]
    }
   ],
   "source": [
    "block = TemporalBlock(\n",
    "    video_dim=32, audio_dim=32, window_size=7, audio_length=12000, video_size=56\n",
    ")\n",
    "video = torch.Tensor(np.random.rand(2, 32, 10, 56, 56))\n",
    "audio = torch.Tensor(np.random.rand(2, 32, 12000))\n",
    "x, y = block(video, audio)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "print(sum(p.numel() for p in block.attention.parameters() if p.requires_grad))\n",
    "for p in block.attention.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa106659-00b0-49c2-9167-f19078604974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:03:29.929550Z",
     "iopub.status.busy": "2023-03-07T13:03:29.928886Z",
     "iopub.status.idle": "2023-03-07T13:03:29.962717Z",
     "shell.execute_reply": "2023-03-07T13:03:29.961315Z",
     "shell.execute_reply.started": "2023-03-07T13:03:29.929502Z"
    },
    "tags": [
     "active-ipynb",
     "style-activity"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 390])\n",
      "torch.Size([2, 10, 1148])\n"
     ]
    }
   ],
   "source": [
    "audio = np.random.rand(2, 10, 1200)\n",
    "y = torch.Tensor(audio)\n",
    "\n",
    "y = nn.Conv1d(10, 1, kernel_size=33, stride=3, padding=1)(y)\n",
    "print(y.shape)\n",
    "\n",
    "audio = np.random.rand(2, 10, 49)\n",
    "y = torch.Tensor(audio)\n",
    "y = nn.ConvTranspose1d(\n",
    "    10, 10, kernel_size=33, stride=15, dilation=13, output_padding=11\n",
    ")(y)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
