{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b263b2-b2fd-49db-b0b8-47889ed5b0e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T01:08:39.949910Z",
     "iopub.status.busy": "2023-04-11T01:08:39.949360Z",
     "iopub.status.idle": "2023-04-11T01:08:39.968692Z",
     "shell.execute_reply": "2023-04-11T01:08:39.967894Z",
     "shell.execute_reply.started": "2023-04-11T01:08:39.949876Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb535fc9-cdf2-49dd-b55c-f3c79c9f5420",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575ba796-5c99-4562-8ae3-b657964baf7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T01:08:40.951557Z",
     "iopub.status.busy": "2023-04-11T01:08:40.950729Z",
     "iopub.status.idle": "2023-04-11T01:08:42.218134Z",
     "shell.execute_reply": "2023-04-11T01:08:42.217521Z",
     "shell.execute_reply.started": "2023-04-11T01:08:40.951511Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from asteroid_filterbanks import Encoder, ParamSincFB\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec1026-5f87-44e9-bd58-6ab92ad1befb",
   "metadata": {
    "tags": [
     "style-activity"
    ]
   },
   "outputs": [],
   "source": [
    "from .block import Block\n",
    "from .style import fuse_audio_video_with_p, fuse_audio_video_with_shuffle, style_aug\n",
    "from .utils import weight_init, LayerNorm, PreEmphasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551b161-ed3d-40ca-9b8f-db162678beb5",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "from block import Block\n",
    "from style import style_aug\n",
    "from utils import fuse_audio_video_with_p, fuse_audio_video_with_shuffle, weight_init, LayerNorm, PreEmphasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9adb02-631f-45cc-b012-fabfb619d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], data_format=\"NCHW\"):\n",
    "        self.t = torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "        self.data_format = data_format\n",
    "\n",
    "    def __call__(self, x, *kargs, **kwargs):\n",
    "        if self.data_format.endswith(\"CHW\"):\n",
    "            return self.t(x)\n",
    "        if self.data_format == \"NCTHW\":\n",
    "            x = torch.transpose(x, 1, 2)\n",
    "            x = self.t(x)\n",
    "            x = torch.transpose(x, 1, 2)\n",
    "            return x\n",
    "        raise ValueError(self.data_format, \"wrong data format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ded6eb-d7bf-4f53-baae-b169afc3e50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T01:11:23.022627Z",
     "iopub.status.busy": "2023-04-11T01:11:23.021742Z",
     "iopub.status.idle": "2023-04-11T01:11:23.062632Z",
     "shell.execute_reply": "2023-04-11T01:11:23.061715Z",
     "shell.execute_reply.started": "2023-04-11T01:11:23.022581Z"
    }
   },
   "outputs": [],
   "source": [
    "class Stem(nn.Sequential):\n",
    "    \"\"\"R(2+1)D stem is different than the default one as it uses separated 3D convolution\"\"\"\n",
    "\n",
    "    def __init__(self, output_channel) -> None:\n",
    "        super().__init__(\n",
    "            nn.Conv3d(\n",
    "                3,\n",
    "                output_channel,\n",
    "                kernel_size=(1, 7, 7),\n",
    "                stride=(1, 2, 2),\n",
    "                padding=(0, 3, 3),\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm3d(output_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c35cce68-b07d-4e0d-9da1-5f2167b838c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:11:46.748081Z",
     "iopub.status.busy": "2022-12-12T03:11:46.747834Z",
     "iopub.status.idle": "2022-12-12T03:11:46.762335Z",
     "shell.execute_reply": "2022-12-12T03:11:46.761837Z",
     "shell.execute_reply.started": "2022-12-12T03:11:46.748067Z"
    }
   },
   "outputs": [],
   "source": [
    "class Stage(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        stage,\n",
    "        video_dim_in,\n",
    "        video_dim_out,\n",
    "        n_frames,\n",
    "        audio_dim_in,\n",
    "        audio_dim_out,\n",
    "        depth,\n",
    "        mlp_ratio=4,\n",
    "        audio_length=None,\n",
    "        video_size=None,\n",
    "        attn_dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if stage == 0:\n",
    "            self.video_downsample = Stem(video_dim_out)\n",
    "        else:\n",
    "            self.video_downsample = nn.Sequential(\n",
    "                nn.Conv3d(\n",
    "                    video_dim_in,\n",
    "                    video_dim_out,\n",
    "                    kernel_size=(1, 7, 7) if stage == 0 else (1, 3, 3),\n",
    "                    stride=(1, 4, 4) if stage == 0 else (1, 2, 2),\n",
    "                    padding=(0, 3, 3) if stage == 0 else (0, 1, 1),\n",
    "                ),\n",
    "                nn.BatchNorm3d(video_dim_out),\n",
    "            )\n",
    "\n",
    "        self.audio_downsample = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                audio_dim_in,\n",
    "                audio_dim_out,\n",
    "                kernel_size=7 if stage == 0 else 3,\n",
    "                stride=4 if stage == 0 else 2,\n",
    "                padding=3 if stage == 0 else 1,\n",
    "            ),\n",
    "            nn.BatchNorm1d(audio_dim_out),\n",
    "        )\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                Block(\n",
    "                    video_dim=video_dim_out,\n",
    "                    n_frames=n_frames,\n",
    "                    audio_dim=audio_dim_out,\n",
    "                    window_size=7,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    audio_length=audio_length,\n",
    "                    video_size=video_size,\n",
    "                    attn_dropout=attn_dropout,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        # self.video_norm = LayerNorm(video_dim_out)\n",
    "        # self.audio_norm = LayerNorm(audio_dim_out)\n",
    "        self.video_norm = nn.BatchNorm3d(video_dim_out)\n",
    "        self.audio_norm = nn.BatchNorm1d(audio_dim_out)\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, video, audio):\n",
    "        x = self.video_downsample(video)\n",
    "        y = self.audio_downsample(audio)\n",
    "        x, y = self.blocks((x, y))\n",
    "        x = self.video_norm(x)\n",
    "        y = self.audio_norm(y)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba2e12-5481-441d-80e7-174a4f1a748e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6f4bf39-d0c4-4510-919f-8de0016c1305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:11:46.763136Z",
     "iopub.status.busy": "2022-12-12T03:11:46.762900Z",
     "iopub.status.idle": "2022-12-12T03:11:46.777861Z",
     "shell.execute_reply": "2022-12-12T03:11:46.776880Z",
     "shell.execute_reply.started": "2022-12-12T03:11:46.763123Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,  # video settings\n",
    "        in_chans=3,\n",
    "        n_frames=10,\n",
    "        audio_freq=16000,  # audio settings\n",
    "        audio_length=48000,\n",
    "        num_classes=2,  # model\n",
    "        out_channels=[32, 64, 128, 256],\n",
    "        mlp_ratios=[4, 4, 4, 4],\n",
    "        depths=[3, 4, 6, 3],\n",
    "        stream_head=True,\n",
    "        cfg=None,\n",
    "        attn_dropout=0.0,\n",
    "        *kargs,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        audio_out_channels, video_out_channels = (\n",
    "            out_channels.copy(),\n",
    "            out_channels.copy(),\n",
    "        )\n",
    "        video_out_channels.insert(0, in_chans)\n",
    "        audio_out_channels.insert(0, 1 if not cfg.preprocess_audio else 256)\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                Stage(\n",
    "                    stage=i,\n",
    "                    video_dim_in=video_out_channels[i],\n",
    "                    video_dim_out=video_out_channels[i + 1],\n",
    "                    n_frames=n_frames,\n",
    "                    audio_dim_in=audio_out_channels[i],\n",
    "                    audio_dim_out=audio_out_channels[i + 1],\n",
    "                    depth=depths[i],\n",
    "                    mlp_ratio=4,\n",
    "                    audio_length=audio_length // (2 ** (i + 2)),\n",
    "                    video_size=img_size // (2 ** (i + 2)),\n",
    "                    attn_dropout=attn_dropout,\n",
    "                )\n",
    "                for i in range(len(depths))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # classificaiton head\n",
    "        final_chanels = video_out_channels[-1]\n",
    "        self.video_head = nn.Linear(final_chanels, num_classes, bias=False)\n",
    "        self.audio_head = nn.Linear(final_chanels, num_classes, bias=False)\n",
    "        self.head = nn.Linear(final_chanels * 2, num_classes, bias=False)\n",
    "\n",
    "        if cfg.normalize:\n",
    "            self.normalizer = Normalize(\n",
    "                data_format=\"NCTHW\", std=cfg.normalize_std, mean=cfg.normalize_mean\n",
    "            )\n",
    "\n",
    "        self.flag_preprocess_audio = False\n",
    "        if cfg.preprocess_audio:\n",
    "            self.flag_preprocess_audio = True\n",
    "            self.audio_preprocess = nn.Sequential(\n",
    "                PreEmphasis(), nn.InstanceNorm1d(1, eps=1e-4, affine=True)\n",
    "            )\n",
    "            self.time_freq_repr = Encoder(\n",
    "                ParamSincFB(n_filters=256, kernel_size=251, stride=1), padding=125\n",
    "            )\n",
    "\n",
    "        K = out_channels[-1]\n",
    "        self.f_video = nn.Sequential(\n",
    "            nn.Conv3d(K, K, 3, padding=1, groups=K),\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            Rearrange(\"b c t h w -> b (c t h w)\"),\n",
    "        )\n",
    "        self.f_audio = nn.Sequential(\n",
    "            nn.Conv1d(K, K, 31, padding=15, groups=K),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            Rearrange(\"b c l -> b (c l)\"),\n",
    "        )\n",
    "        self.style_aug = cfg.style_aug\n",
    "        if self.style_aug:\n",
    "            self.f_video_adv = nn.Sequential(\n",
    "                nn.Conv3d(K, K, 3, padding=1, groups=K),\n",
    "                nn.AdaptiveAvgPool3d(1),\n",
    "                Rearrange(\"b c t h w -> b (c t h w)\"),\n",
    "            )\n",
    "            self.f_audio_adv = nn.Sequential(\n",
    "                nn.Conv1d(K, K, 31, padding=15, groups=K),\n",
    "                nn.AdaptiveAvgPool1d(1),\n",
    "                Rearrange(\"b c l -> b (c l)\"),\n",
    "            )\n",
    "            self.video_head_adv = nn.Linear(video_out_channels[-1], num_classes)\n",
    "            self.audio_head_adv = nn.Linear(audio_out_channels[-1], num_classes)\n",
    "            self.head_adv = nn.Linear(video_out_channels[-1], num_classes)\n",
    "\n",
    "        # Final, init all weights\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def preprocess_audio(self, x):\n",
    "        x = self.audio_preprocess(x)\n",
    "        x = torch.abs(self.time_freq_repr(x))\n",
    "        x = torch.log(x + 1e-6)\n",
    "        x = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "        return x\n",
    "\n",
    "    def adv_params(self):\n",
    "        params = []\n",
    "        for layer in self.adv_modules():\n",
    "            for m in layer.modules():\n",
    "                params += [p for p in m.parameters()]\n",
    "        return params\n",
    "\n",
    "    def style_params(self):\n",
    "        params = []\n",
    "        for m in [\n",
    "            self.f_video_adv,\n",
    "            self.f_audio_adv,\n",
    "            self.video_head_adv,\n",
    "            self.audio_head_adv,\n",
    "        ]:\n",
    "            params += [p for p in m.parameters()]\n",
    "        return params\n",
    "\n",
    "    def adv_modules(self):\n",
    "        if self.flag_preprocess_audio:\n",
    "            return [self.stages, self.audio_preprocess, self.time_freq_repr]\n",
    "        else:\n",
    "            return [self.stages]\n",
    "\n",
    "    def style_modules(self):\n",
    "        if self.style_aug:\n",
    "            return [\n",
    "                self.f_video_adv,\n",
    "                self.f_audio_adv,\n",
    "                self.video_head_adv,\n",
    "                self.audio_head_adv,\n",
    "            ]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def forward(self, video, audio, cls_token=False, train=False, grad_cam=False):\n",
    "        B, C, T, H, W = video.shape\n",
    "        B, C, L = audio.shape\n",
    "\n",
    "        x, y = video, audio\n",
    "        res = {}\n",
    "\n",
    "        if hasattr(self, \"normalizer\"):\n",
    "            x = self.normalizer(x)\n",
    "        if self.flag_preprocess_audio:\n",
    "            y = self.preprocess_audio(y)\n",
    "\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            x, y = stage(x, y)\n",
    "            # print(x.shape)\n",
    "            if grad_cam:\n",
    "                res[\"org_x_%d\" % i] = x\n",
    "\n",
    "        if self.style_aug and train:\n",
    "            x_style, x_adv = style_aug(x, dims=[-3, -2, -1])\n",
    "            y_style, y_adv = style_aug(y, dims=[-1])\n",
    "            res[\"x\"] = self.f_video(x_style)\n",
    "            res[\"y\"] = self.f_audio(y_style)\n",
    "\n",
    "            x_adv = self.f_video_adv(x_adv.detach())\n",
    "            y_adv = self.f_audio_adv(y_adv.detach())\n",
    "            res[\"video_label_adv\"] = self.video_head_adv(x_adv)\n",
    "            res[\"audio_label_adv\"] = self.audio_head_adv(y_adv)\n",
    "        else:\n",
    "            res[\"x\"] = self.f_video(x)\n",
    "            res[\"y\"] = self.f_audio(y)\n",
    "\n",
    "        res[\"video_label\"] = self.video_head(res[\"x\"])\n",
    "        res[\"audio_label\"] = self.audio_head(res[\"y\"])\n",
    "        res[\"z\"] = torch.concat([res[\"x\"], res[\"y\"]], dim=-1)\n",
    "\n",
    "        if self.cfg.shuffle_av and train:\n",
    "            res[\"z_adv\"], res[\"shuffle_ids\"] = fuse_audio_video_with_shuffle(\n",
    "                res[\"x\"], res[\"y\"]\n",
    "            )\n",
    "            res[\"total_label_adv\"] = self.head(res[\"z_adv\"])\n",
    "\n",
    "        res[\"total_label\"] = self.head(res[\"z\"])\n",
    "        return res"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
