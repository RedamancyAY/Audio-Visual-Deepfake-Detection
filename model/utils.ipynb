{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1fa250-c78c-46ee-a492-66f3b27da501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:33.129552Z",
     "iopub.status.busy": "2023-04-17T05:47:33.128887Z",
     "iopub.status.idle": "2023-04-17T05:47:33.151641Z",
     "shell.execute_reply": "2023-04-17T05:47:33.151136Z",
     "shell.execute_reply.started": "2023-04-17T05:47:33.129483Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c92d549-28f5-4728-8f30-c37978157ccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T13:13:20.982418Z",
     "iopub.status.busy": "2023-02-23T13:13:20.981551Z",
     "iopub.status.idle": "2023-02-23T13:13:21.712742Z",
     "shell.execute_reply": "2023-02-23T13:13:21.711488Z",
     "shell.execute_reply.started": "2023-02-23T13:13:20.982299Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b14b9-b96a-4a00-891e-3b758e3eb78d",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2275df98-a47a-4382-b64c-a4c3cb33c445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:34.465971Z",
     "iopub.status.busy": "2023-04-17T05:47:34.465461Z",
     "iopub.status.idle": "2023-04-17T05:47:36.398829Z",
     "shell.execute_reply": "2023-04-17T05:47:36.398214Z",
     "shell.execute_reply.started": "2023-04-17T05:47:34.465941Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from torch import einsum, nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09b5ca-bc9b-49df-bd36-3a5581dfae67",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fb0e4-29c2-4250-af96-cdf7b6e05036",
   "metadata": {},
   "source": [
    "## 参数初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68c3209-686b-404c-b89e-19c601a0a1e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:36.638938Z",
     "iopub.status.busy": "2023-04-17T05:47:36.638486Z",
     "iopub.status.idle": "2023-04-17T05:47:36.661962Z",
     "shell.execute_reply": "2023-04-17T05:47:36.661417Z",
     "shell.execute_reply.started": "2023-04-17T05:47:36.638915Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    # if isinstance(m, (nn.Conv2d, nn.Linear, nn.Conv3d)):\n",
    "    # nn.init.xavier_normal_(m.weight, gain=math.sqrt(2.0))\n",
    "    # nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2.0))\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        fan_out //= m.groups\n",
    "        m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, (nn.Conv3d, nn.Conv1d)):\n",
    "        nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2.0))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm3d, nn.LayerNorm)):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc788f-fbe6-4ca2-9472-3ab9b0808ab1",
   "metadata": {},
   "source": [
    "## Multi_Head_Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "868ae928-8217-4f4c-8c2d-9accd41a8189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:50:09.970582Z",
     "iopub.status.busy": "2023-04-17T05:50:09.970085Z",
     "iopub.status.idle": "2023-04-17T05:50:09.996574Z",
     "shell.execute_reply": "2023-04-17T05:50:09.995986Z",
     "shell.execute_reply.started": "2023-04-17T05:50:09.970554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _multi_head_attention(q, k, v, heads=1, dropout=None):\n",
    "    q, k, v = map(\n",
    "        lambda mat: rearrange(mat, \"b n (h d) -> (b h) n d\", h=heads), (q, k, v)\n",
    "    )\n",
    "    scale = q.shape[-1] ** -0.5\n",
    "    qkT = einsum(\"b n d, b m d->b n m\", q, k) * scale\n",
    "    attention = dropout(qkT.softmax(dim=-1))\n",
    "    attention = einsum(\"b n m, b m d->b n d\", attention, v)\n",
    "    attention = rearrange(attention, \"(b h) n d -> b n (h d)\", h=heads)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abb00337-136d-4b62-9407-72f56e467099",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:50:40.177412Z",
     "iopub.status.busy": "2023-04-17T05:50:40.176922Z",
     "iopub.status.idle": "2023-04-17T05:50:40.201331Z",
     "shell.execute_reply": "2023-04-17T05:50:40.200830Z",
     "shell.execute_reply.started": "2023-04-17T05:50:40.177384Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings,\n",
    "        embed_dim,\n",
    "        num_heads=1,\n",
    "        QKV=False,\n",
    "        projection=False,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(num_embeddings)\n",
    "        self.PE = PositionEmbedding(\n",
    "            num_embeddings=num_embeddings, embedding_dim=embed_dim\n",
    "        )\n",
    "        self.num_heads = num_heads\n",
    "        self.QKV = QKV\n",
    "        self.projection = projection\n",
    "        if QKV:\n",
    "            self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        if projection:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim, bias=False), nn.Dropout(dropout)\n",
    "            )\n",
    "        self.apply(self._init_weights)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.PE(x)\n",
    "        x = self.norm(x)\n",
    "        if self.QKV:\n",
    "            q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        else:\n",
    "            q, k, v = x, x, x\n",
    "        x = _multi_head_attention(q, k, v, heads=self.num_heads, dropout=self.dropout)\n",
    "        if self.projection:\n",
    "            x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84202196-2405-41fc-8256-3764794fef72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T13:13:20.982418Z",
     "iopub.status.busy": "2023-02-23T13:13:20.981551Z",
     "iopub.status.idle": "2023-02-23T13:13:21.712742Z",
     "shell.execute_reply": "2023-02-23T13:13:21.711488Z",
     "shell.execute_reply.started": "2023-02-23T13:13:20.982299Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2795b2-d62d-4837-ada0-0b649f1459b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T13:16:59.344514Z",
     "iopub.status.busy": "2023-02-23T13:16:59.343175Z",
     "iopub.status.idle": "2023-02-23T13:16:59.356353Z",
     "shell.execute_reply": "2023-02-23T13:16:59.355013Z",
     "shell.execute_reply.started": "2023-02-23T13:16:59.344464Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self._norm = nn.LayerNorm(dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 4:\n",
    "            x = rearrange(x, \"b c h w -> b h w c\")\n",
    "            x = self._norm(x)\n",
    "            x = rearrange(x, \"b h w c -> b c h w\")\n",
    "        elif x.ndim == 5:\n",
    "            x = rearrange(x, \"b c t h w -> b t h w c\")\n",
    "            x = self._norm(x)\n",
    "            x = rearrange(x, \"b t h w c -> b c t h w\")\n",
    "        elif x.ndim == 3:\n",
    "            x = rearrange(x, \"b c l -> b l c\")\n",
    "            x = self._norm(x)\n",
    "            x = rearrange(x, \"b l c -> b c l\")\n",
    "        return x\n",
    "class PositionEmbedding(nn.Module):\n",
    "\n",
    "    MODE_EXPAND = \"MODE_EXPAND\"\n",
    "    MODE_ADD = \"MODE_ADD\"\n",
    "    MODE_CONCAT = \"MODE_CONCAT\"\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim, mode=MODE_ADD):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mode = mode\n",
    "        if self.mode == self.MODE_EXPAND:\n",
    "            self.weight = nn.Parameter(\n",
    "                torch.Tensor(num_embeddings * 2 + 1, embedding_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n",
    "        self.reset_parameters()\n",
    "        # print(\"PositionEmbedding, weight shape is \", self.weight.shape)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_normal_(self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == self.MODE_EXPAND:\n",
    "            indices = (\n",
    "                torch.clamp(x, -self.num_embeddings, self.num_embeddings)\n",
    "                + self.num_embeddings\n",
    "            )\n",
    "            return F.embedding(indices.type(torch.LongTensor), self.weight)\n",
    "        batch_size, seq_len = x.size()[:2]\n",
    "        # print(x.shape, seq_len, self.num_embeddings, self.embedding_dim)\n",
    "        embeddings = self.weight[:seq_len, :].view(1, seq_len, self.embedding_dim)\n",
    "        if self.mode == self.MODE_ADD:\n",
    "            return x + embeddings\n",
    "        if self.mode == self.MODE_CONCAT:\n",
    "            return torch.cat((x, embeddings.repeat(batch_size, 1, 1)), dim=-1)\n",
    "        raise NotImplementedError(\"Unknown mode: %s\" % self.mode)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"num_embeddings={}, embedding_dim={}, mode={}\".format(\n",
    "            self.num_embeddings,\n",
    "            self.embedding_dim,\n",
    "            self.mode,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ee165-b217-4b79-a62c-345059f0b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PreEmphasis(torch.nn.Module):\n",
    "    def __init__(self, coef: float = 0.97) -> None:\n",
    "        super().__init__()\n",
    "        self.coef = coef\n",
    "        # make kernel\n",
    "        # In pytorch, the convolution operation uses cross-correlation. So, filter is flipped.\n",
    "        self.register_buffer(\n",
    "            \"flipped_filter\",\n",
    "            torch.FloatTensor([-self.coef, 1.0]).unsqueeze(0).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        assert x.ndim in [2, 3]\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        # reflect padding to match lengths of in/out\n",
    "        x = F.pad(x, (1, 0), \"reflect\")\n",
    "        return F.conv1d(x, self.flipped_filter)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
